{
 "metadata": {
  "name": "",
  "signature": "sha256:a8068ee3fb0ed020b3971a02194ddb7bf159f0dbb04e145669ba636b9a57a8b1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ignore\n",
      "Title: Lasagne implementation of the adam SGD update rule\n",
      "Slug: adamlasagne\n",
      "Date: 02-19-2015\n",
      "Category:\n",
      "Tags: lasagne theano\n",
      "Author: Eben Olson\n",
      "Summary:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def adam(loss, all_params, learning_rate=0.0002, beta1=0.1, beta2=0.001, epsilon=1e-8, l_decay=1e-8):\n",
      "    \"\"\"\n",
      "    Adam: A Method for Stochastic Optimization\n",
      "    Diederik P. Kingma, Jimmy Lei Ba\n",
      "    arXiv:1412.6980v2 [cs.LG] 17 Jan 2015\n",
      "    \"\"\"\n",
      "    beta1_factor = theano.shared(np.dtype(theano.config.floatX).type((1-beta1)))\n",
      "    beta2_factor = theano.shared(np.dtype(theano.config.floatX).type((1-beta2)))\n",
      "    l_decay_factor = theano.shared(np.dtype(theano.config.floatX).type(1))\n",
      "\n",
      "    all_m = [theano.shared(np.zeros(param.get_value().shape, dtype=theano.config.floatX)) for param in all_params]\n",
      "    all_v = [theano.shared(np.zeros(param.get_value().shape, dtype=theano.config.floatX)) for param in all_params]\n",
      "\n",
      "    all_grads = theano.grad(loss, all_params)\n",
      "\n",
      "    updates = []\n",
      "\n",
      "    updates.append((l_decay_factor, l_decay*l_decay_factor))\n",
      "    updates.append((beta1_factor, (1-beta1)*beta1_factor))\n",
      "    updates.append((beta2_factor, (1-beta2)*beta2_factor))\n",
      "\n",
      "    beta1_t = 1-(1-beta1)*l_decay_factor\n",
      "\n",
      "    for param_i, grad_i, m_i, v_i in zip(all_params, all_grads, all_m, all_v):\n",
      "        m_i_new = beta1_t*grad_i + (1-beta1_t)*m_i\n",
      "        updates.append((m_i, m_i_new))\n",
      "\n",
      "        v_i_new = beta2*grad_i**2 + (1-beta2)*v_i\n",
      "        updates.append((v_i, v_i_new))\n",
      "\n",
      "        param_i_new = param_i - learning_rate*m_i_new/(1-beta1_factor)/(T.sqrt(v_i_new/(1-beta2_factor))+epsilon)\n",
      "        updates.append((param_i, param_i_new))\n",
      "\n",
      "    return updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}